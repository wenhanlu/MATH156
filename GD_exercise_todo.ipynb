{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to import the necessary libaries\n",
    "import numpy as np\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to classify the iris dataset, RUN THE NEXT CELL WITHOUT MODIFICATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading and preprocessing the data, don't change\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "names = iris['target_names']\n",
    "feature_names = iris['feature_names']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# One hot encoding\n",
    "enc = OneHotEncoder()\n",
    "Y = enc.fit_transform(y[:, np.newaxis]).toarray()\n",
    "\n",
    "# Scale data to have mean 0 and variance 1 \n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data set into training and testing\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_scaled, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "Y_train = Y_train.T\n",
    "Y_test = Y_test.T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPLETE THE CODE IN THE FOLLOWING CELL, THEN RUN IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gradient(W, X_batch, t_batch):\n",
    "    Y = softmax(W @ X_batch, axis=0)  # this is our model for the prediction\n",
    "    N = X_batch.shape[1]  # number of points in batch\n",
    "    K = W.shape[0]  # number of classes\n",
    "    M = W.shape[1]  # number of features\n",
    "\n",
    "    G = np.zeros((K, M))  # this will store the gradient, the dimensions are K: number of classes, M: number of features\n",
    "\n",
    "    # complete the function and calculate the gradient according to the formula in the exercises\n",
    "    for n in range(N):\n",
    "        x_n = X_batch[:, n].reshape(-1, 1)\n",
    "        t_n = t_batch[:, n].reshape(-1, 1)\n",
    "        y_n = Y[:, n].reshape(-1, 1)\n",
    "\n",
    "        G += np.outer(y_n - t_n, x_n)\n",
    "\n",
    "    return G \n",
    "\n",
    "\n",
    "def logisticGD(X_train, t_train, batch_size, l_rate=0.1, tol=1e-5, epochs=10):\n",
    "    K = t_train.shape[0]\n",
    "    N = X_train.shape[1]\n",
    "    M = X_train.shape[0]\n",
    "    \n",
    "    W = np.random.rand(K, M)\n",
    "    \n",
    "    norm_G = float('inf')\n",
    "    \n",
    "    n_batches = N // batch_size\n",
    "\n",
    "    epoch = 1\n",
    "    \n",
    "    while epoch <= epochs and norm_G > tol:\n",
    "        indices = np.random.permutation(N)\n",
    "        X_shuffle = X_train[:, indices]\n",
    "        t_shuffle = t_train[:, indices]\n",
    "\n",
    "        for j in range(n_batches):\n",
    "            start_idx = j * batch_size\n",
    "            end_idx = (j + 1) * batch_size\n",
    "            X_batch = X_shuffle[:, start_idx:end_idx]\n",
    "            t_batch = t_shuffle[:, start_idx:end_idx]\n",
    "            G = Gradient(W, X_batch, t_batch)\n",
    "            W -= l_rate * G\n",
    "            norm_G = np.linalg.norm(G)\n",
    "            epoch += 1\n",
    "        \n",
    "    return W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPLETE THE CODE IN THE FOLLOWING CELL, THEN RUN IT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMPLETE THE FOLLOWING CELL\n",
    "epochs = 30\n",
    "\n",
    "#Training the models with GD, SGD or mini-batch GD\n",
    "\n",
    "#Define the correct value for batch_size \n",
    "### GD\n",
    "batch_size = X_train.shape[1] #batch size for GD\n",
    "W_GD = logisticGD(X_train, Y_train, batch_size = batch_size, epochs = epochs, tol = 1e-5, l_rate = 0.001)\n",
    "\n",
    "### SGD\n",
    "batch_size = 1 #batch size for SGD\n",
    "W_SGD = logisticGD(X_train, Y_train, batch_size = batch_size, epochs = epochs, tol = 1e-5, l_rate = 0.001)\n",
    "\n",
    "### Mini batch GD\n",
    "batch_size = 10 #batch size for mini batch GD\n",
    "W_MGD = logisticGD(X_train, Y_train, batch_size = batch_size, epochs = epochs, tol = 1e-5, l_rate = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy GD: 0.8583333333333333\n",
      "Test Accuracy GD: 0.7333333333333333\n",
      "Training Accuracy SGD: 0.7833333333333333\n",
      "Test Accuracy SGD: 0.6666666666666666\n",
      "Training Accuracy mini batch GD: 0.825\n",
      "Test Accuracy mini batch GD: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy on the training set\n",
    "Y_train_pred = softmax(W_GD @ (X_train))\n",
    "train_predictions = np.argmax(Y_train_pred, axis=0)\n",
    "train_true = np.argmax(Y_train, axis = 0)\n",
    "train_accuracy = np.mean(train_predictions == train_true)\n",
    "print(\"Training Accuracy GD:\", train_accuracy)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "Y_test_pred = softmax(W_GD @ (X_test))\n",
    "test_predictions = np.argmax(Y_test_pred, axis=0)\n",
    "test_true = np.argmax(Y_test, axis = 0)\n",
    "test_accuracy = np.mean(test_predictions == test_true)\n",
    "print(\"Test Accuracy GD:\", test_accuracy)\n",
    "\n",
    "# Calculate accuracy on the training set\n",
    "Y_train_pred = softmax(W_SGD @ (X_train))\n",
    "train_predictions = np.argmax(Y_train_pred, axis=0)\n",
    "train_true = np.argmax(Y_train, axis = 0)\n",
    "train_accuracy = np.mean(train_predictions == train_true)\n",
    "print(\"Training Accuracy SGD:\", train_accuracy)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "Y_test_pred = softmax(W_SGD @ (X_test))\n",
    "test_predictions = np.argmax(Y_test_pred, axis=0)\n",
    "test_true = np.argmax(Y_test, axis = 0)\n",
    "test_accuracy = np.mean(test_predictions == test_true)\n",
    "print(\"Test Accuracy SGD:\", test_accuracy)\n",
    "\n",
    "\n",
    "# Calculate accuracy on the training set\n",
    "Y_train_pred = softmax(W_MGD @ (X_train))\n",
    "train_predictions = np.argmax(Y_train_pred, axis=0)\n",
    "train_true = np.argmax(Y_train, axis = 0)\n",
    "train_accuracy = np.mean(train_predictions == train_true)\n",
    "print(\"Training Accuracy mini batch GD:\", train_accuracy)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "Y_test_pred = softmax(W_MGD @ (X_test))\n",
    "test_predictions = np.argmax(Y_test_pred, axis=0)\n",
    "test_true = np.argmax(Y_test, axis = 0)\n",
    "test_accuracy = np.mean(test_predictions == test_true)\n",
    "print(\"Test Accuracy mini batch GD:\", test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
